{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport os","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = tf.keras.utils.get_file(\n'nietzsche.txt',\norigin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')","execution_count":4,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n606208/600901 [==============================] - 0s 1us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = open(path).read()\nprint(len(text))\nprint(text[:500])","execution_count":5,"outputs":[{"output_type":"stream","text":"600893\nPREFACE\n\n\nSUPPOSING that Truth is a woman--what then? Is there not ground\nfor suspecting that all philosophers, in so far as they have been\ndogmatists, have failed to understand women--that the terrible\nseriousness and clumsy importunity with which they have usually paid\ntheir addresses to Truth, have been unskilled and unseemly methods for\nwinning a woman? Certainly she has never allowed herself to be won; and\nat present every kind of dogma stands with sad and discouraged mien--IF,\nindeed, it s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 60\nstep = 3\nsentences = []\nnext_chars = []\nfor i in range(0, len(text)-maxlen, step):\n    sentences.append(text[i:i+maxlen])\n    next_chars.append(text[i+maxlen])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsentences[0]","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is the'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_chars[0]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"'r'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"200278"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_len = len(chars)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_indices = dict((char, chars.index(char)) for char in chars)\nchar_indices","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"{'\\n': 0,\n ' ': 1,\n '!': 2,\n '\"': 3,\n \"'\": 4,\n '(': 5,\n ')': 6,\n ',': 7,\n '-': 8,\n '.': 9,\n '0': 10,\n '1': 11,\n '2': 12,\n '3': 13,\n '4': 14,\n '5': 15,\n '6': 16,\n '7': 17,\n '8': 18,\n '9': 19,\n ':': 20,\n ';': 21,\n '=': 22,\n '?': 23,\n 'A': 24,\n 'B': 25,\n 'C': 26,\n 'D': 27,\n 'E': 28,\n 'F': 29,\n 'G': 30,\n 'H': 31,\n 'I': 32,\n 'J': 33,\n 'K': 34,\n 'L': 35,\n 'M': 36,\n 'N': 37,\n 'O': 38,\n 'P': 39,\n 'Q': 40,\n 'R': 41,\n 'S': 42,\n 'T': 43,\n 'U': 44,\n 'V': 45,\n 'W': 46,\n 'X': 47,\n 'Y': 48,\n 'Z': 49,\n '[': 50,\n ']': 51,\n '_': 52,\n 'a': 53,\n 'b': 54,\n 'c': 55,\n 'd': 56,\n 'e': 57,\n 'f': 58,\n 'g': 59,\n 'h': 60,\n 'i': 61,\n 'j': 62,\n 'k': 63,\n 'l': 64,\n 'm': 65,\n 'n': 66,\n 'o': 67,\n 'p': 68,\n 'q': 69,\n 'r': 70,\n 's': 71,\n 't': 72,\n 'u': 73,\n 'v': 74,\n 'w': 75,\n 'x': 76,\n 'y': 77,\n 'z': 78,\n 'Æ': 79,\n 'ä': 80,\n 'æ': 81,\n 'é': 82,\n 'ë': 83}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[100]","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"array([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False,  True,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[100].shape","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"(60, 84)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\nmodel = tf.keras.models.Sequential()\nmodel.add(layers.LSTM(128, input_shape=(maxlen, vocab_len)))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(layers.Dense(vocab_len, activation=\"softmax\"))","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":18,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 128)               109056    \n_________________________________________________________________\ndense (Dense)                (None, 84)                10836     \n=================================================================\nTotal params: 119,892\nTrainable params: 119,892\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import random\nimport sys\nfor epoch in range(0, 60):\n    print(\"Epoch \", epoch)\n    f = open(\"epoch_{}.txt\".format(epoch), \"w\")\n    f.write(\"Epoch {}\\n\".format(epoch))\n    model.fit(x,y, batch_size=128)\n    start_index = random.randint(0, len(text) - maxlen - 1)\n    generated_text = text[start_index: start_index + maxlen]\n    print('--- Generating with seed: \"' + generated_text + '\"')\n    f.write(\"Seed {}\\n\".format(generated_text))\n    for temperature in [0.2, 0.5, 1.0, 1.2]:\n        print('------ temperature:', temperature)\n        f.write(\"Temperature {}\\n\".format(temperature))\n        #print(generated_text)\n        for i in range(400):\n            sampled = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(generated_text):\n                sampled[0, t, char_indices[char]] = 1.\n            preds = model.predict(sampled, verbose=0)[0]\n            next_index = sample(preds, temperature)\n            next_char = chars[next_index]\n            generated_text += next_char\n            generated_text = generated_text[1:]\n           # sys.stdout.write(next_char)\n            f.write(next_char)\n    f.close()","execution_count":22,"outputs":[{"output_type":"stream","text":"Epoch  0\n1565/1565 [==============================] - 10s 7ms/step - loss: 2.2131\n--- Generating with seed: \"To REVERSE all estimates of value--THAT is what they\nhad to \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  1\n1565/1565 [==============================] - 10s 6ms/step - loss: 2.0674\n--- Generating with seed: \"ld-renunciation, and will-renunciation, both\nsymptoms perhap\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  2\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.9681\n--- Generating with seed: \"They set themselves\nbefore the eyes of all not alone as mode\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  3\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.8946\n--- Generating with seed: \" whoever, with an Asiatic and super-Asiatic\neye, has actuall\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  4\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.8362\n--- Generating with seed: \"ediately?\" Or: \"What is the use of any hasty\nhypotheses? It \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  5\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.7892\n--- Generating with seed: \"pared and experienced side by side; which\nwas impossible at \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  6\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.7469\n--- Generating with seed: \" training of the intellectual powers were insisted upon by t\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  7\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.7104\n--- Generating with seed: \"s matter; the development and rapid\nflourishing of German ph\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  8\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.6783\n--- Generating with seed: \"th the lapse of time, bore off the palm of\nvictory. The man \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  9\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.6496\n--- Generating with seed: \" older, ampler, and more radically ingrained\npropensity oppo\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  10\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.6233\n--- Generating with seed: \"ven it with some mischief. In effect, the\nold English vice c\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  11\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.5990\n--- Generating with seed: \"o the whole of\nthis exuberant and eccentric movement (which \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  12\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.5771\n--- Generating with seed: \"ment. But whoever\nconsiders the fundamental impulses of man \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  13\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.5568\n--- Generating with seed: \"ill is not only a complex of sensation and\nthinking, but it \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  14\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.5387\n--- Generating with seed: \"pproval of ignorance:\nas that which is all necessary accordi\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  15\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.5213\n--- Generating with seed: \"st the general exhaustion of their will to\nlive (their nerve\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  16\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.5048\n--- Generating with seed: \"their belly before everything that is massive. And so also i\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  17\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.4901\n--- Generating with seed: \"low himself to be degraded, robbed,\ndeceived, and exploited \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  18\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.4761\n--- Generating with seed: \"e an open ear wherever there is talk without indignation. Fo\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  19\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.4629\n--- Generating with seed: \" comfort to the sufferers, courage to\nthe oppressed and desp\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  20\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.4505\n--- Generating with seed: \"t that the consensus gentium amounts to an\nabsurdity.\n\n\n111\n\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  21\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.4387\n--- Generating with seed: \"in inversion, nor anything at once so\ndreadful, questioning,\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  22\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.4275\n--- Generating with seed: \"ideas in this mode\nof looking at things, a disbelief in all \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  23\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.4168\n--- Generating with seed: \"strustful and henceforth for ever\nuseless.--In the domain of\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  24\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.4072\n--- Generating with seed: \"n,\nsinfulness, unworthiness: he sees in them merely the flit\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  25\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3975\n--- Generating with seed: \" belongs to the tendency and\nfundamental taste of democracy,\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  26\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3885\n--- Generating with seed: \"hy should be manifested but men should take care not\nto feel\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  27\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.3799\n--- Generating with seed: \"invades the domain of the benefactor and gets\nsatisfaction t\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  28\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.3711\n--- Generating with seed: \"rari_, the spheres of invariable causation,\nnecessity and ir\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  29\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.3633\n--- Generating with seed: \"an in the assertions of the founder of the church. So, too, \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  30\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.3556\n--- Generating with seed: \"ence of acts\nwhich inspire it in many other men. It is a ver\"\n------ temperature: 0.2\n","name":"stdout"},{"output_type":"stream","text":"------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  31\n1565/1565 [==============================] - 10s 7ms/step - loss: 1.3483\n--- Generating with seed: \"s to \"conception\" or \"notion.\"\n\n\n30\n\n=Evil Habits in Reachin\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  32\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3418\n--- Generating with seed: \" which is the danger of dangers for highly developed\nand wea\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  33\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3341\n--- Generating with seed: \", our laughter itself may have a\nfuture!\n\n224. The historica\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  34\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3272\n--- Generating with seed: \"teful.\" Let us finally confess it, that what is\nmost difficu\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  35\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3212\n--- Generating with seed: \" before a general depression of European intelligence.\n\nWhat\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  36\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3144\n--- Generating with seed: \" by\nconflagrations, catch up every cooling and extinguishing\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  37\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3218\n--- Generating with seed: \" foreign elements reveals itself in a strong\ntendency to ass\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  38\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.3056\n--- Generating with seed: \"l anew\nthe fascination of the spectacle, to yield to it, sat\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  39\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2977\n--- Generating with seed: \"fine and\ntrained perceptions, it requires a surplus, a surpl\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  40\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2941\n--- Generating with seed: \"ht to the\nBIG period, we modern men, who are short of breath\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  41\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2891\n--- Generating with seed: \"vements of his soul and body; indeed, not even the desire fo\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  42\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2831\n--- Generating with seed: \"e so proud, although they should\nall be ashamed of it. If a \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  43\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2789\n--- Generating with seed: \" course of our whole lives. No one knows to what lengths\ncir\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  44\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2744\n--- Generating with seed: \"e same freedom\nthat Æschylus and Aristophanes evinced and wi\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  45\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2691\n--- Generating with seed: \"toics, and let us send to its\nhelp whatever devilry we have \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  46\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2640\n--- Generating with seed: \"\n\n=Being Unjust is Essential.=--All judgments of the value o\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  47\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2596\n--- Generating with seed: \"ething of a slave, though certainly the sublimest sort of sl\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  48\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2556\n--- Generating with seed: \"udes of heredity, have escaped proper development.\nThey show\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  49\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2507\n--- Generating with seed: \"onclusion that a person is superficial--they WISH to mislead\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  50\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2471\n--- Generating with seed: \"s;\nsupposing, however, that he does not voluntarily take all\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  51\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2440\n--- Generating with seed: \"h ventures to do so,\nhas thereby alone placed itself beyond \"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  52\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2389\n--- Generating with seed: \"because he gazes into\nthis clear mirror, that his own self s\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  53\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2363\n--- Generating with seed: \"ut of the way!\nFlee into concealment! And have your masks an\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  54\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2320\n--- Generating with seed: \"mands, for the maintenance of a definite\nmode of life For ex\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  55\n1565/1565 [==============================] - 12s 7ms/step - loss: 1.2283\n--- Generating with seed: \"es it finds itself always in paradise.\n\n\n125\n\n=Irreligiousne\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  56\n1565/1565 [==============================] - 12s 7ms/step - loss: 1.2256\n--- Generating with seed: \"t act. \"Let no more Jews come in! And shut\nthe doors, especi\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  57\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2205\n--- Generating with seed: \"ght\nto enter--that is philosophy in its last throes, an end,\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  58\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2179\n--- Generating with seed: \"GAINST \"youth.\"--A decade later, and one\ncomprehends that al\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\nEpoch  59\n1565/1565 [==============================] - 11s 7ms/step - loss: 1.2154\n--- Generating with seed: \"ents, and prefers to try\nconclusions with the artificial, as\"\n------ temperature: 0.2\n------ temperature: 0.5\n------ temperature: 1.0\n------ temperature: 1.2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch = 1\n_f = open(\"epoch_{}.txt\".format(epoch), \"r\")","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 10):\n    _f = open(\"epoch_{}.txt\".format(i), \"w\")\n    _f.write(\"epoch {}\\n\".format(i))\n    sr = \"rohan\"\n    for i in sr:\n        _f.write(i)\n    _f.close()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":2,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\npath_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')","execution_count":1,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n1122304/1115394 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read, then decode for py2 compat.\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n# length of text is the number of characters in it\nprint ('Length of text: {} characters'.format(len(text)))","execution_count":2,"outputs":[{"output_type":"stream","text":"Length of text: 1115394 characters\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The unique characters in the file\nvocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab)))","execution_count":3,"outputs":[{"output_type":"stream","text":"65 unique characters\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text)//(seq_length+1)\n\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n  print(idx2char[i.numpy()])","execution_count":5,"outputs":[{"output_type":"stream","text":"F\ni\nr\ns\nt\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n  print(repr(''.join(idx2char[item.numpy()])))","execution_count":6,"outputs":[{"output_type":"stream","text":"'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","execution_count":8,"outputs":[{"output_type":"stream","text":"Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\nTarget data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for input_example_batch, target_example_batch in dataset.take(1):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","execution_count":13,"outputs":[{"output_type":"stream","text":"(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":14,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           16640     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 65)            66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","execution_count":15,"outputs":[{"output_type":"stream","text":"Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.173251\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss=loss)\nhistory = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])","execution_count":17,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n172/172 [==============================] - 8s 44ms/step - loss: 2.6692\nEpoch 2/10\n172/172 [==============================] - 7s 44ms/step - loss: 1.9716\nEpoch 3/10\n172/172 [==============================] - 7s 43ms/step - loss: 1.7030\nEpoch 4/10\n172/172 [==============================] - 8s 44ms/step - loss: 1.5521\nEpoch 5/10\n172/172 [==============================] - 7s 43ms/step - loss: 1.4642\nEpoch 6/10\n172/172 [==============================] - 7s 43ms/step - loss: 1.4030\nEpoch 7/10\n172/172 [==============================] - 7s 43ms/step - loss: 1.3575\nEpoch 8/10\n172/172 [==============================] - 7s 43ms/step - loss: 1.3193\nEpoch 9/10\n172/172 [==============================] - 7s 43ms/step - loss: 1.2852\nEpoch 10/10\n172/172 [==============================] - 8s 44ms/step - loss: 1.2537\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = 2000\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions / temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))\nprint(generate_text(model, start_string=u\"ROMEO: \"))","execution_count":19,"outputs":[{"output_type":"stream","text":"ROMEO: we!\nThe wedding-shit Would incense. But for your hagrant;\nFor him, my good mortak from the ground, Thomouhe by the winds for God us;\nTo weep it the duke,\nDo nd git so.\n\nEDWARD:\nSoul he out, I can well knock thy bear?\n\nAUFIDIUS:\nHow fartoch--\n\nCOMINIUS:\nDo not this death:\nExcellent glack for counte heartily grace:\nThou desire you as his foot fewname wors:\nOnce more Edward:\nThe world shall be full of noble heart.\n\nPROSPERO:\nWhere is thy foels,\nI think thyself?\n\nAGHOMAS:\nHave to yourself cle-kit your father's carub\nof presence that I were, gentle maided:\nToo duty, god in this foul fire,\nThat could perform with wars light to him:\nWhile it is postery. Wed in thy face, I can tell you.\n\nCOMINIUS:\nThe host shall make to blaw.\n\nPAULINA:\nHow fails upon your addies,\nTo do to let him well, in the adventious steed\nWas to sing and to stay point of torgure in the queen.\n\nBALNANIO:\nFear too woman.\n\nCOMINIUS:\nHide me on my faith, nor the crown\nIn all is ELIZABETH:\nThe mutinius, Cagens: I was wine noble\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
